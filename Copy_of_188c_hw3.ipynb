{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 188c_hw3.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mikofarin12/Amazon/blob/main/Copy_of_188c_hw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqDFkRi5Wmf7"
      },
      "source": [
        "#188C Homework 3: Finetuning CNN for Image Classification\n",
        "In this homework, we will finetune a CNN model to classify whether people in the images are wearing masks.\n",
        "\n",
        "Any changes won't be saved on this example notebook. Please edit on your own copy. \n",
        "\n",
        "Please submit the pdf version of your notebook(on the top left menu, click File --> Print to get the pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPkd7-FTWxqd"
      },
      "source": [
        "##Step1: environment setup\n",
        "In this tutorial, we will use PyTorch deep learning framework. For training acceleration, we can use free GPU offered in Google Colab. To change to GPU setting, click \"Runtime --> change runtime type\" and change acceleration type to GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NahWG6KiTm02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecbb6c81-54dc-4348-de94-86300f7019bf"
      },
      "source": [
        "from __future__ import print_function \n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch Version:  1.8.0+cu101\n",
            "Torchvision Version:  0.9.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIH6_gl1Xij4"
      },
      "source": [
        "##Step 2: model input set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci76Zku_aYKf"
      },
      "source": [
        "Please upload the zip file https://drive.google.com/drive/folders/1J7zq8j03w1R4DzcIFiLIDuOgCxlOeugy?usp=sharing into your own Google drive and put it in a folder called mask_classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-NHUUKCZ_T4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e35b8fd-c899-4998-a72d-f2d1ded2eb1f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPTDG41HbnmG"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "with ZipFile('/content/gdrive/MyDrive/mask_classification/mask_image_set.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "   zipObj.extractall()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BD8xmIBAXb_t"
      },
      "source": [
        "# path to the ImageFolder\n",
        "data_dir = './mask_image_set'\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 2\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 64\n",
        "\n",
        "# Number of epochs to train for \n",
        "num_epochs = 15\n",
        "\n",
        "# When False, we finetune the whole model, when True we only update the reshaped layer params\n",
        "feature_extract = True\n",
        "\n",
        "# image size for the network input\n",
        "input_size = 224"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5v47HS5gNlS"
      },
      "source": [
        "In PyTorch, data is organized using DataLoader and Dataset modules\n",
        "\n",
        "Datasets: the abstract structure that organize all the images and labels\n",
        "\n",
        "Dataloader: the generator to yeild data batch for model training at each step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8dgPnaGgA4m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23899599-64c3-4c29-8aed-ed4a61ce83b0"
      },
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Create training and validation datasets\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
        "\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzG032auZAZQ"
      },
      "source": [
        "##Step 3: model initialization\n",
        "There are lots of deep models with hundreds of layers trained on Imagenet, a large dataset including images of 1000 classes. We consider models trained on this large dataset have already gained plenty of visual knowledge, therefore, after we use our own data to finetune model, hopefully the model will learn to deal with our new task combining knowledge gained from task-specific new data and its previous visual knowledge trained from Imagenet.\n",
        "\n",
        "To initialize the model, we can take advantage of TorchVision, a package saving plenty of deep model parameters. As mentioned above, the model is learned to classify 1000 classes. Here we only want to classify two classes, therefore, after downloading the model, we will change the dimension of the last layer to two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN6ctIeca_mq"
      },
      "source": [
        "# if feature_extract=True, this function will freeze all layers except the last layer\n",
        "def set_parameter_requires_grad(model, feature_extract):\n",
        "    if feature_extract:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX6fPI2GadH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b7c07b6-3cfe-4590-dd1f-36232e828acc"
      },
      "source": [
        "model_ft = models.vgg11_bn(pretrained=True) # pretrained=True will initialize the model with parameters learned from Imagenet\n",
        "set_parameter_requires_grad(model_ft, feature_extract)\n",
        "num_ftrs = model_ft.classifier[6].in_features\n",
        "model_ft.classifier[6] = nn.Linear(num_ftrs,2  ) # change the dimension of the last layer to two\n",
        "model_ft.to(device)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): ReLU(inplace=True)\n",
              "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (24): ReLU(inplace=True)\n",
              "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dusu1rXfmZHX"
      },
      "source": [
        "##Step 4: optimization tools\n",
        "\n",
        "In deep learning, we use loss metrics to evalute how close between the predicted label and the grouth truth. A smaller loss means a better performance. \n",
        "\n",
        "To minimize the loss at each time step, we will use the optimizer to compute the gradients and backpropagate through the network.\n",
        "\n",
        "Here we will use sotochastic gradient descent as our optimizer and cross entropy as our loss metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1tLJLckkv1O"
      },
      "source": [
        "# you can play with lr to find the best accuracy\n",
        "optimizer = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss() "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoZoE0ngofqJ"
      },
      "source": [
        "##Step 5: training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71GDiMFkoiyU"
      },
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6NmF-5bpeWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f5f4d7c-7a36-4a95-f155-dfc3c1abd3d8"
      },
      "source": [
        "model_ft = train_model(model_ft, dataloaders_dict, criterion, optimizer, num_epochs)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/14\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 0.6895 Acc: 0.5633\n",
            "val Loss: 0.5533 Acc: 0.7350\n",
            "\n",
            "Epoch 1/14\n",
            "----------\n",
            "train Loss: 0.4932 Acc: 0.7750\n",
            "val Loss: 0.4187 Acc: 0.8200\n",
            "\n",
            "Epoch 2/14\n",
            "----------\n",
            "train Loss: 0.4166 Acc: 0.8050\n",
            "val Loss: 0.3716 Acc: 0.8350\n",
            "\n",
            "Epoch 3/14\n",
            "----------\n",
            "train Loss: 0.3750 Acc: 0.8517\n",
            "val Loss: 0.3483 Acc: 0.8650\n",
            "\n",
            "Epoch 4/14\n",
            "----------\n",
            "train Loss: 0.3457 Acc: 0.8550\n",
            "val Loss: 0.3324 Acc: 0.8700\n",
            "\n",
            "Epoch 5/14\n",
            "----------\n",
            "train Loss: 0.3413 Acc: 0.8417\n",
            "val Loss: 0.3173 Acc: 0.8800\n",
            "\n",
            "Epoch 6/14\n",
            "----------\n",
            "train Loss: 0.3274 Acc: 0.8600\n",
            "val Loss: 0.3040 Acc: 0.8800\n",
            "\n",
            "Epoch 7/14\n",
            "----------\n",
            "train Loss: 0.3289 Acc: 0.8667\n",
            "val Loss: 0.3029 Acc: 0.8700\n",
            "\n",
            "Epoch 8/14\n",
            "----------\n",
            "train Loss: 0.3119 Acc: 0.8717\n",
            "val Loss: 0.2958 Acc: 0.8800\n",
            "\n",
            "Epoch 9/14\n",
            "----------\n",
            "train Loss: 0.3090 Acc: 0.8700\n",
            "val Loss: 0.2908 Acc: 0.8800\n",
            "\n",
            "Epoch 10/14\n",
            "----------\n",
            "train Loss: 0.3178 Acc: 0.8550\n",
            "val Loss: 0.2892 Acc: 0.8800\n",
            "\n",
            "Epoch 11/14\n",
            "----------\n",
            "train Loss: 0.3324 Acc: 0.8583\n",
            "val Loss: 0.2848 Acc: 0.8850\n",
            "\n",
            "Epoch 12/14\n",
            "----------\n",
            "train Loss: 0.3308 Acc: 0.8717\n",
            "val Loss: 0.2853 Acc: 0.8850\n",
            "\n",
            "Epoch 13/14\n",
            "----------\n",
            "train Loss: 0.3212 Acc: 0.8583\n",
            "val Loss: 0.2746 Acc: 0.8850\n",
            "\n",
            "Epoch 14/14\n",
            "----------\n",
            "train Loss: 0.2907 Acc: 0.8767\n",
            "val Loss: 0.2780 Acc: 0.8850\n",
            "\n",
            "Training complete in 3m 25s\n",
            "Best val Acc: 0.885000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypB6wG--eevW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f053d2-468c-4991-b6f8-910e4211b075"
      },
      "source": [
        "# test the model using test data\n",
        "# load test dataset and create test dataloader as in step 2\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "image_datasets_test = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'test']}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict_test = {x: torch.utils.data.DataLoader(image_datasets_test[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train','test']}\n",
        "\n",
        "def test_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to test mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'test' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'test':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best test Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "model_ft = test_model(model_ft, dataloaders_dict_test, criterion, optimizer, num_epochs)\n",
        "\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/14\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 0.7388 Acc: 0.4850\n",
            "test Loss: 0.7071 Acc: 0.4800\n",
            "\n",
            "Epoch 1/14\n",
            "----------\n",
            "train Loss: 0.7255 Acc: 0.5167\n",
            "test Loss: 0.7067 Acc: 0.4800\n",
            "\n",
            "Epoch 2/14\n",
            "----------\n",
            "train Loss: 0.7350 Acc: 0.4917\n",
            "test Loss: 0.7063 Acc: 0.4750\n",
            "\n",
            "Epoch 3/14\n",
            "----------\n",
            "train Loss: 0.7337 Acc: 0.4883\n",
            "test Loss: 0.7079 Acc: 0.4700\n",
            "\n",
            "Epoch 4/14\n",
            "----------\n",
            "train Loss: 0.7348 Acc: 0.4967\n",
            "test Loss: 0.7085 Acc: 0.4750\n",
            "\n",
            "Epoch 5/14\n",
            "----------\n",
            "train Loss: 0.7375 Acc: 0.4717\n",
            "test Loss: 0.7078 Acc: 0.4700\n",
            "\n",
            "Epoch 6/14\n",
            "----------\n",
            "train Loss: 0.7370 Acc: 0.4700\n",
            "test Loss: 0.7063 Acc: 0.4700\n",
            "\n",
            "Epoch 7/14\n",
            "----------\n",
            "train Loss: 0.7290 Acc: 0.4967\n",
            "test Loss: 0.7058 Acc: 0.4750\n",
            "\n",
            "Epoch 8/14\n",
            "----------\n",
            "train Loss: 0.7350 Acc: 0.4633\n",
            "test Loss: 0.7072 Acc: 0.4750\n",
            "\n",
            "Epoch 9/14\n",
            "----------\n",
            "train Loss: 0.7360 Acc: 0.5083\n",
            "test Loss: 0.7068 Acc: 0.4750\n",
            "\n",
            "Epoch 10/14\n",
            "----------\n",
            "train Loss: 0.7313 Acc: 0.4900\n",
            "test Loss: 0.7074 Acc: 0.4700\n",
            "\n",
            "Epoch 11/14\n",
            "----------\n",
            "train Loss: 0.7358 Acc: 0.4600\n",
            "test Loss: 0.7077 Acc: 0.4700\n",
            "\n",
            "Epoch 12/14\n",
            "----------\n",
            "train Loss: 0.7424 Acc: 0.4750\n",
            "test Loss: 0.7073 Acc: 0.4850\n",
            "\n",
            "Epoch 13/14\n",
            "----------\n",
            "train Loss: 0.7269 Acc: 0.4683\n",
            "test Loss: 0.7092 Acc: 0.4750\n",
            "\n",
            "Epoch 14/14\n",
            "----------\n",
            "train Loss: 0.7391 Acc: 0.4717\n",
            "test Loss: 0.7088 Acc: 0.4800\n",
            "\n",
            "Training complete in 3m 44s\n",
            "Best test Acc: 0.485000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}